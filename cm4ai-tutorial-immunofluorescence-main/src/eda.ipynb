{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2f5f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import json\n",
    "from collections import Counter\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"../data/raw\"\n",
    "CHANNELS = [\"blue\", \"green\", \"red\", \"yellow\"]\n",
    "\n",
    "def collect_image_paths(base_path=BASE_PATH):\n",
    "    records = []\n",
    "    for treatment_folder in os.listdir(base_path):\n",
    "        treatment_path = os.path.join(base_path, treatment_folder)\n",
    "        if not os.path.isdir(treatment_path):\n",
    "            continue\n",
    "\n",
    "        treatment = treatment_folder.split(\"-\")[-1].lower()\n",
    "\n",
    "        image_dict = {}\n",
    "        for channel in CHANNELS:\n",
    "            channel_path = os.path.join(treatment_path, channel)\n",
    "            for img_path in glob(os.path.join(channel_path, \"*.jpg\")):\n",
    "                # Extract base ID (strip _blue, _red, etc.)\n",
    "                basename = os.path.basename(img_path).replace(f\"_{channel}.jpg\", \"\")\n",
    "                image_dict.setdefault(basename, {\"id\": basename, \"treatment\": treatment})\n",
    "                image_dict[basename][channel] = img_path\n",
    "\n",
    "        records.extend(image_dict.values())\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6789fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rocrate_metadata_with_antibodies(base_path=BASE_PATH):\n",
    "    metadata_records = []\n",
    "\n",
    "    for treatment_folder in os.listdir(base_path):\n",
    "        crate_path = os.path.join(base_path, treatment_folder, \"ro-crate-metadata.json\")\n",
    "        if not os.path.isfile(crate_path):\n",
    "            continue\n",
    "\n",
    "        with open(crate_path, \"r\") as f:\n",
    "            crate = json.load(f)\n",
    "\n",
    "        # --- Build antibody/stain index ---\n",
    "        antibody_index = {}\n",
    "        for entry in crate.get(\"@graph\", []):\n",
    "            if entry.get(\"@type\") == \"BioChemEntity\":\n",
    "                stain_id = entry[\"@id\"]\n",
    "                identifiers = entry.get(\"identifier\", [])\n",
    "                if isinstance(identifiers, dict):\n",
    "                    identifiers = [identifiers]\n",
    "\n",
    "                id_map = {i.get(\"name\"): i.get(\"value\") for i in identifiers}\n",
    "\n",
    "                antibody_index[stain_id] = {\n",
    "                    \"name\": entry.get(\"name\"),\n",
    "                    \"description\": entry.get(\"description\"),\n",
    "                    \"hpa_id\": id_map.get(\"HPA Antibody ID\"),\n",
    "                    \"ensembl\": id_map.get(\"ENSEMBL\"),\n",
    "                    \"uniprot\": id_map.get(\"Uniprot\"),\n",
    "                    \"pubchem\": id_map.get(\"PubChem\"),\n",
    "                    \"subcellular_location\": (\n",
    "                        entry.get(\"isLocatedInSubcellularLocation\", {}).get(\"name\")\n",
    "                        if isinstance(entry.get(\"isLocatedInSubcellularLocation\"), dict)\n",
    "                        else None\n",
    "                    )\n",
    "                }\n",
    "\n",
    "        # --- Process each dataset (image) entry ---\n",
    "        for entry in crate.get(\"@graph\", []):\n",
    "            if entry.get(\"@type\") != \"EVI:Dataset\":\n",
    "                continue\n",
    "\n",
    "            content_url = entry.get(\"contentUrl\", \"\")\n",
    "            filename = os.path.basename(content_url.replace(\"file://\", \"\")).strip(\"/\")\n",
    "            if not filename.endswith(\".jpg\"):\n",
    "                continue\n",
    "\n",
    "            base_id = filename.replace(\".jpg\", \"\").rsplit(\"_\", 1)[0]\n",
    "            channel = filename.replace(\".jpg\", \"\").rsplit(\"_\", 1)[-1].lower()\n",
    "\n",
    "            stain_ref = entry.get(\"usedStain\", {}).get(\"@id\", \"\")\n",
    "            stain_key = stain_ref.split(\"/\")[-1].replace(\"stain-\", \"\")\n",
    "            ab_meta = antibody_index.get(stain_ref, {})\n",
    "\n",
    "            metadata_records.append({\n",
    "                \"id\": base_id,\n",
    "                \"channel\": channel,\n",
    "                \"antibody_stain\": stain_key,\n",
    "                \"antibody_name\": ab_meta.get(\"name\"),\n",
    "                \"antibody_hpa_id\": ab_meta.get(\"hpa_id\"),\n",
    "                \"antibody_ensembl\": ab_meta.get(\"ensembl\"),\n",
    "                \"antibody_uniprot\": ab_meta.get(\"uniprot\"),\n",
    "                \"antibody_pubchem\": ab_meta.get(\"pubchem\"),\n",
    "                \"subcellular_location\": ab_meta.get(\"subcellular_location\"),\n",
    "                \"cell_line\": entry.get(\"usedCellLine\", {}).get(\"@id\", \"\").split(\"/\")[-1].replace(\"cell-line-\", \"\"),\n",
    "                \"treatment\": entry.get(\"usedTreatment\", {}).get(\"@id\", \"\").split(\"/\")[-1].replace(\"treatment-\", \"\"),\n",
    "                \"description\": entry.get(\"description\", \"\"),\n",
    "                \"filename\": filename\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(metadata_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b6d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_lookup_ensembl_symbols(ensembl_ids, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Look up gene symbols from Ensembl using batched POST requests.\n",
    "    Returns a dict {ensembl_id: gene_symbol}\n",
    "    \"\"\"\n",
    "    url = \"https://rest.ensembl.org/lookup/id\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    id_to_symbol = {}\n",
    "\n",
    "    for i in range(0, len(ensembl_ids), batch_size):\n",
    "        batch = ensembl_ids[i:i + batch_size]\n",
    "        payload = {\"ids\": batch}\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=payload)\n",
    "            if response.status_code == 200:\n",
    "                results = response.json()\n",
    "                for eid, info in results.items():\n",
    "                    id_to_symbol[eid] = info.get(\"display_name\", None)\n",
    "            else:\n",
    "                print(f\"⚠️ Error {response.status_code}: {response.text}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Request failed for batch starting at {i}: {e}\")\n",
    "    \n",
    "    return id_to_symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08331bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image_gene_node_attributes(df_merged, base_output_dir=\"data/raw\"):\n",
    "    # Filter to green channel (protein target)\n",
    "    df_green = df_merged[df_merged[\"channel\"] == \"green\"].copy()\n",
    "\n",
    "    # Normalize treatment label: \"control\" becomes \"untreated\"\n",
    "    df_green[\"treatment\"] = df_green[\"treatment\"].replace(\"control\", \"untreated\")\n",
    "\n",
    "    # Drop exact duplicates across key fields\n",
    "    df_green = df_green.drop_duplicates(subset=[\"id\", \"treatment\", \"antibody_hpa_id\", \"antibody_ensembl\"])\n",
    "\n",
    "    # Group by treatment\n",
    "    treatments = df_green[\"treatment\"].dropna().unique()\n",
    "\n",
    "    for treatment in treatments:\n",
    "        df_t = df_green[df_green[\"treatment\"] == treatment]\n",
    "\n",
    "        df_out = pd.DataFrame({\n",
    "            \"name\": df_t[\"antibody_name\"],\n",
    "            \"represents\": \"ensembl:\" + df_t[\"antibody_ensembl\"].fillna(\"\"),\n",
    "            \"ambiguous\": df_t[\"antibody_hpa_id\"],\n",
    "            \"antibody\": df_t[\"antibody_hpa_id\"],\n",
    "            \"filename\": df_t[\"id\"].astype(str) + \"_\",\n",
    "            \"imageurl\": \"no image url found\"\n",
    "        })\n",
    "\n",
    "        unique_ensembl_ids = (\n",
    "            df_out[\"represents\"]\n",
    "            .dropna()\n",
    "            .str.replace(\"ensembl:\", \"\", regex=False)\n",
    "            .loc[lambda s: s.str.match(r\"ENSG\\d+\")]  # keep only valid Ensembl Gene IDs\n",
    "            .unique()\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        ensembl_to_name = batch_lookup_ensembl_symbols(unique_ensembl_ids)\n",
    "\n",
    "        df_out[\"name\"] = (\n",
    "            df_out[\"represents\"]\n",
    "            .str.replace(\"ensembl:\", \"\", regex=False)\n",
    "            .map(ensembl_to_name)\n",
    "        )\n",
    "\n",
    "        df_out[\"name\"] = df_out[\"name\"].fillna(\"NEGATIVE\")\n",
    "\n",
    "        # Save to the appropriate treatment folder\n",
    "        treatment_folder = os.path.join(base_output_dir, treatment)\n",
    "        os.makedirs(treatment_folder, exist_ok=True)\n",
    "\n",
    "        out_path = os.path.join(treatment_folder, \"1_image_gene_node_attributes.tsv\")\n",
    "        df_out.to_csv(out_path, sep=\"\\t\", index=False)\n",
    "\n",
    "        print(f\"✅ Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d499b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_multichannel_image(row):\n",
    "    \"\"\"\n",
    "    Loads a 4-channel immunofluorescence image from separate grayscale files.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): A row from df_images with keys: blue, green, red, yellow.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: H x W x 4 array with channels in the order [blue, green, red, yellow]\n",
    "    \"\"\"\n",
    "    img_channels = []\n",
    "    for ch in [\"blue\", \"green\", \"red\", \"yellow\"]:\n",
    "        path = row[ch]\n",
    "        img = Image.open(path).convert(\"L\")  # Load as 8-bit grayscale\n",
    "        img_array = np.array(img)\n",
    "        img_channels.append(img_array)\n",
    "\n",
    "    stacked = np.stack(img_channels, axis=-1)  # Shape: H x W x 4\n",
    "    return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a3bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats_row(row):\n",
    "    img = load_multichannel_image(row)\n",
    "    results = []\n",
    "    for i, ch in enumerate(CHANNELS):\n",
    "        ch_data = img[..., i]\n",
    "        results.append({\n",
    "            \"id\": row[\"id\"],\n",
    "            \"treatment\": row[\"treatment\"],\n",
    "            \"channel\": ch,\n",
    "            \"mean\": np.mean(ch_data),\n",
    "            \"std\": np.std(ch_data),\n",
    "            \"min\": np.min(ch_data),\n",
    "            \"max\": np.max(ch_data),\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def compute_channel_stats_parallel(df, n_jobs=-1):\n",
    "    results = Parallel(n_jobs=n_jobs, backend=\"threading\")(\n",
    "        delayed(compute_stats_row)(row) for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Parallel channel stats\")\n",
    "    )\n",
    "    # Flatten nested list\n",
    "    flat_results = [item for sublist in results for item in sublist]\n",
    "    return pd.DataFrame(flat_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a6b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary_report(df_merged, n_jobs=4):\n",
    "    print(\"🧬🔬 CM4AI Immunofluorescence Dataset Summary\\n\" + \"=\"*45, flush=True)\n",
    "\n",
    "    # 1. Number of treatments\n",
    "    n_treatments = df_merged[\"treatment\"].nunique()\n",
    "    print(f\"\\n💊 Number of treatments: {n_treatments}\", flush=True)\n",
    "    for cond, count in df_merged[\"treatment\"].value_counts().items():\n",
    "        print(f\"  - {cond}: {count} image-channel combinations\", flush=True)\n",
    "\n",
    "    # 2. Number of samples (unique image IDs) per treatment\n",
    "    print(\"\\n🧪 Number of unique samples per treatment:\", flush=True)\n",
    "    samples_per_treatment = (\n",
    "        df_merged[[\"id\", \"treatment\"]]\n",
    "        .drop_duplicates()\n",
    "        .groupby(\"treatment\")\n",
    "        .size()\n",
    "    )\n",
    "    for cond, count in samples_per_treatment.items():\n",
    "        print(f\"  - {cond}: {count} samples\", flush=True)\n",
    "\n",
    "    # 3. Image size distribution (parallelized)\n",
    "    print(\"\\n🖼 Image size distribution:\", flush=True)\n",
    "\n",
    "    # Reconstruct wide format for loading multichannel images\n",
    "    df_channels = df_merged[[\"id\", \"channel\", \"filepath\"]].drop_duplicates()\n",
    "    df_shapes = df_channels.pivot(index=\"id\", columns=\"channel\", values=\"filepath\").reset_index()\n",
    "    df_treatments = df_merged[[\"id\", \"treatment\"]].drop_duplicates()\n",
    "    df_shapes = df_shapes.merge(df_treatments, on=\"id\", how=\"left\")\n",
    "\n",
    "    def safe_load_shape(row):\n",
    "        try:\n",
    "            img = load_multichannel_image(row)\n",
    "            return img.shape[:2]\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ Error loading image for ID {row['id']}: {e}\", flush=True)\n",
    "            return None\n",
    "\n",
    "    print(\"🔄 Computing image shapes in parallel...\", flush=True)\n",
    "    shapes = Parallel(n_jobs=n_jobs, backend=\"threading\")(\n",
    "        delayed(safe_load_shape)(row) for _, row in tqdm(df_shapes.iterrows(), total=len(df_shapes))\n",
    "    )\n",
    "    df_shapes[\"shape\"] = shapes\n",
    "    shape_counts = Counter([s for s in shapes if s is not None])\n",
    "    for shape, count in shape_counts.items():\n",
    "        print(f\"  - {shape[0]}x{shape[1]}: {count} composite/multi-channel images\", flush=True)\n",
    "\n",
    "    # 4. Green channel antibody diversity\n",
    "    green_df = df_merged[df_merged[\"channel\"] == \"green\"]\n",
    "    unique_green = sorted(set(green_df[\"antibody_hpa_id\"].dropna().tolist()))\n",
    "    print(f\"\\n🟩 Number of unique antibodies in green channel (protein target): {len(unique_green)}\", flush=True)\n",
    "\n",
    "    # 5. Red, Blue, Yellow antibody/stain names with icons\n",
    "    print(\"\\n🎯 Antibodies/stains used in other channels:\", flush=True)\n",
    "\n",
    "    channel_icons = {\n",
    "        \"red\": \"🟥\",\n",
    "        \"blue\": \"🟦\",\n",
    "        \"yellow\": \"🟨\"\n",
    "    }\n",
    "\n",
    "    for ch in [\"red\", \"blue\", \"yellow\"]:\n",
    "        ch_df = df_merged[df_merged[\"channel\"] == ch]\n",
    "        unique_ab = sorted(set(\n",
    "            ch_df[\"antibody_hpa_id\"].dropna().tolist() +\n",
    "            ch_df[\"antibody_name\"].dropna().tolist()\n",
    "        ))\n",
    "        icon = channel_icons.get(ch, \"🔹\")\n",
    "        print(f\"\\n  {icon} {ch.upper()} channel antibodies/stains ({len(unique_ab)}):\", flush=True)\n",
    "        for ab in unique_ab:\n",
    "            print(f\"    - {ab}\", flush=True)\n",
    "\n",
    "    print(\"\\n✅ Summary complete.\\n\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d8a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images = collect_image_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f8e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df_images[df_images.duplicated(subset=\"id\", keep=False)].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bbb166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = load_rocrate_metadata_with_antibodies()\n",
    "df_images_melted = df_images.melt(\n",
    "    id_vars=[\"id\"],  # remove \"treatment\" here\n",
    "    value_vars=[\"blue\", \"green\", \"red\", \"yellow\"],\n",
    "    var_name=\"channel\",\n",
    "    value_name=\"filepath\"\n",
    ")\n",
    "\n",
    "df_merged = df_images_melted.merge(df_meta, on=[\"id\", \"channel\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038d89b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove once image downloader integrated\n",
    "save_image_gene_node_attributes(df_merged, base_output_dir=BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fc515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_summary_report(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae60730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = compute_channel_stats_parallel(df_images)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(data=df_stats, x=\"channel\", y=\"mean\", hue=\"treatment\")\n",
    "plt.title(\"Mean Channel Intensity by Treatment\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cm4ai-if-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
